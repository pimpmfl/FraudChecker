{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fcbf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afad06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/mlproject22\" if os.path.exists(\"/data/mlproject22\") else \".\"\n",
    "train_data = pd.read_csv(os.path.join(path, \"transactions.csv.zip\"))\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = train_data.drop(columns=\"Class\")\n",
    "y = train_data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a56b9ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC Score: 0.873395738665613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45490\n",
      "           1       0.97      0.75      0.84        79\n",
      "\n",
      "    accuracy                           1.00     45569\n",
      "   macro avg       0.98      0.87      0.92     45569\n",
      "weighted avg       1.00      1.00      1.00     45569\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['random_forest_scaler.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLASS WEIGHTS METHOD (high precision on frauds)\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = train_data.drop(columns=\"Class\")\n",
    "y = train_data[\"Class\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Perform data normalization using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = dict(1 / y_train.value_counts())\n",
    "\n",
    "# Train Random Forest classifier with class weights\n",
    "rf_classifier = RandomForestClassifier(random_state=123, class_weight=class_weights)\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the AUC-ROC score\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC-ROC Score:\", auc_roc)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained model and scaler\n",
    "joblib.dump(rf_classifier, \"random_forest_params.pkl\")\n",
    "joblib.dump(scaler, \"random_forest_scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed37a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "def leader_board_predict_fn(values):\n",
    "    \n",
    "    # Load the trained model parameters\n",
    "    rf_classifier = joblib.load(\"/home/csaz7668/random_forest_params.pkl\")\n",
    "    scaler = joblib.load(\"/home/csaz7668/random_forest_scaler.pkl\")\n",
    "    X = values\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    # Predict the likelihood of fraud (decision function values) for each transaction\n",
    "    decision_function_values = rf_classifier.predict_proba(X_scaled)[:, 1]  # Get the probability of the positive class\n",
    "    print(decision_function_values)\n",
    "    return decision_function_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f67b5f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (227845, 30)\n",
      "Shape of y_test: (227845,)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Train Dataset Score: 0.9935252189198022\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Test Dataset Score: 0.9527033889667291\n"
     ]
    }
   ],
   "source": [
    "def get_score():\n",
    "    \"\"\"\n",
    "    Function to compute scores for train and test datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import pathlib\n",
    "\n",
    "    try:\n",
    "        path = \"/data/mlproject22\" if os.path.exists(\"/data/mlproject22\") else \".\"\n",
    "        test_data = pd.read_csv(os.path.join(path, \"transactions.csv.zip\"))\n",
    "        X_test = test_data.drop(columns = \"Class\")\n",
    "        y_test = test_data[\"Class\"]\n",
    "        print(\"Shape of X_test:\", X_test.shape)\n",
    "        print(\"Shape of y_test:\", y_test.shape)\n",
    "        decision_function_values = leader_board_predict_fn(X_test)\n",
    "        assert decision_function_values.shape == (X_test.shape[0],)\n",
    "        dataset_score = roc_auc_score(y_test, decision_function_values)\n",
    "        assert dataset_score >= 0.0 and dataset_score <= 1.0\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))\n",
    "        dataset_score = float(\"nan\")\n",
    "    print(f\"Train Dataset Score: {dataset_score}\")\n",
    "\n",
    "    import os\n",
    "    import pwd\n",
    "    import time\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    user_id = pwd.getpwuid( os.getuid() ).pw_name\n",
    "    curtime = time.time()\n",
    "    dt_now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    try:\n",
    "        HIDDEN_DATASET_PATH = os.path.expanduser(\"/data/mlproject22-test-data\")\n",
    "        test_data = pd.read_csv(os.path.join(HIDDEN_DATASET_PATH,\"transactions_scoreboard.csv.zip\"))\n",
    "        X_test = test_data.drop(columns=[\"Class\"])\n",
    "        y_test = test_data[\"Class\"]\n",
    "        decision_function_values = leader_board_predict_fn(X_test)\n",
    "        hiddendataset_score = roc_auc_score(y_test, decision_function_values)\n",
    "        print(f\"Test Dataset Score: {hiddendataset_score}\")\n",
    "        score_dict = dict(\n",
    "            score_hidden=hiddendataset_score,\n",
    "            score_train=dataset_score,\n",
    "            unixtime=curtime,\n",
    "            user=user_id,\n",
    "            dt=dt_now,\n",
    "            comment=\"\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "        score_dict = dict(\n",
    "            score_hidden=float(\"nan\"),\n",
    "            score_train=dataset_score,\n",
    "            unixtime=curtime,\n",
    "            user=user_id,\n",
    "            dt=dt_now,\n",
    "            comment=err\n",
    "        )\n",
    "\n",
    "    #if list(pathlib.Path(os.getcwd()).parents)[0].name == 'source':\n",
    "    #    print(\"we are in the source directory... replacing values.\")\n",
    "    #    print(pd.DataFrame([score_dict]))\n",
    "    #    score_dict[\"score_hidden\"] = -1\n",
    "    #    score_dict[\"score_train\"] = -1\n",
    "    #    print(\"new values:\")\n",
    "    #    print(pd.DataFrame([score_dict]))\n",
    "\n",
    "    pd.DataFrame([score_dict]).to_csv(\"transactions_test.csv\", index=False)\n",
    "    \n",
    "get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4d5f3502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC Score: 0.9524828102434532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     45490\n",
      "           1       0.05      0.94      0.09        79\n",
      "\n",
      "    accuracy                           0.97     45569\n",
      "   macro avg       0.52      0.95      0.54     45569\n",
      "weighted avg       1.00      0.97      0.98     45569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UNDERSAMPLING NON-FRAUDULENT TRANSACTIONS METHOD (low precision on frauds)\n",
    "'''\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Perform data normalization using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Undersample the majority class\n",
    "X_train_resampled, y_train_resampled = resample(X_train_scaled[y_train == 0], y_train[y_train == 0],\n",
    "                                               n_samples=np.sum(y_train == 1), random_state=42)\n",
    "\n",
    "# Concatenate the minority class with the undersampled majority class\n",
    "X_train_resampled = np.concatenate((X_train_scaled[y_train == 1], X_train_resampled), axis=0)\n",
    "y_train_resampled = np.concatenate((y_train[y_train == 1], y_train_resampled), axis=0)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the AUC-ROC score\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC-ROC Score:\", auc_roc)\n",
    "\n",
    "# Save parameters & scaler\n",
    "# joblib.dump(rf_classifier, \"random_forest_params.pkl\")\n",
    "# joblib.dump(scaler, \"random_forest_scaler.pkl\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e19a59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227845\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf1a5584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5.495683140892829e-06, 1: 0.0031746031746031746}\n"
     ]
    }
   ],
   "source": [
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb65d296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    181961\n",
      "1       315\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b644d289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "59\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred[y_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#plt.scatter(non_fraud_indices, y_pred[y_test == 0], c='green', label='Predicted Non-Fraud')\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mscatter(fraud_indices, y_pred[y_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m], c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Fraud\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData Point\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass (0: Non-Fraud, 1: Fraud)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fraud_indices = y_test[y_test == 1].index\n",
    "non_fraud_indices = y_test[y_test == 0].index\n",
    "\n",
    "#plt.scatter(fraud_indices, y_test[y_test == 1], c='blue', label='Actual Fraud')\n",
    "#plt.scatter(non_fraud_indices, y_test[y_test == 0], c='orange', label='Actual Non-Fraud')\n",
    "\n",
    "print(y_pred[y_test == 0].sum())\n",
    "print(y_pred[y_test == 1].sum())\n",
    "\n",
    "#plt.scatter(non_fraud_indices, y_pred[y_test == 0], c='green', label='Predicted Non-Fraud')\n",
    "plt.scatter(fraud_indices, y_pred[y_test == 1], c='red', label='Predicted Fraud')\n",
    "\n",
    "plt.xlabel('Data Point')\n",
    "plt.ylabel('Class (0: Non-Fraud, 1: Fraud)')\n",
    "plt.title('Binary Classification Results')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f7a5ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[485, 1165, 1249, 1927, 3134, 3555, 3887, 4707, 4724, 4885, 4897, 5149, 6248, 6771, 8512, 8718, 8788, 9321, 9628, 10361, 11062, 11652, 11855, 12046, 12376, 13627, 14217, 14446, 16067, 16158, 16487, 16719, 16959, 17750, 18694, 19778, 20431, 20968, 22537, 23085, 23420, 23895, 25370, 25432, 26090, 26894, 27424, 28269, 30956, 32349, 33638, 34035, 34098, 34440, 36131, 37255, 38665, 39195, 39978, 42833, 44057]\n",
      "List has a length of: 61\n"
     ]
    }
   ],
   "source": [
    "predicted_index = []\n",
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i] == 1):\n",
    "        predicted_index.append(i)\n",
    "print(predicted_index)\n",
    "print(\"List has a length of:\", len(predicted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b27130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct frauds detected by the model: 59\n",
      "Non-frauds predicted as frauds by the model: 2\n",
      "Frauds predicted as non-frauds by the model: 20\n",
      "Total frauds in the test dataset: 79\n"
     ]
    }
   ],
   "source": [
    "found = 0\n",
    "false_values = 0\n",
    "frauds_as_non_frauds = 0\n",
    "comparision_indices = 0\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test.iloc[i] == 1 and i in predicted_index):\n",
    "        found += 1\n",
    "    if(y_test.iloc[i] == 0 and i in predicted_index):\n",
    "        false_values += 1\n",
    "    if(y_test.iloc[i] == 1 and i not in predicted_index):\n",
    "        frauds_as_non_frauds += 1\n",
    "        \n",
    "print(\"Correct frauds detected by the model:\", found)\n",
    "print(\"Non-frauds predicted as frauds by the model:\", false_values)\n",
    "print(\"Frauds predicted as non-frauds by the model:\", frauds_as_non_frauds)\n",
    "print(\"Total frauds in the test dataset:\", y_test[y_test == 1].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1358e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive values: 59\n",
      "True negative values: 45488\n",
      "False positive values: 2\n",
      "False negative values: 20\n",
      "Total sum: 45569\n",
      "Accuracy: 0.9995172156509908\n"
     ]
    }
   ],
   "source": [
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for t, p in zip(y_test, y_pred):\n",
    "    if(t == 1 and p == 1):\n",
    "        tp += 1\n",
    "    if(t == 0 and p == 0):\n",
    "        tn += 1\n",
    "    if(t == 0 and p == 1):\n",
    "        fp += 1\n",
    "    if(t == 1 and p == 0):\n",
    "        fn += 1\n",
    "print(\"True positive values:\", tp)\n",
    "print(\"True negative values:\", tn)\n",
    "print(\"False positive values:\", fp)\n",
    "print(\"False negative values:\", fn)\n",
    "print(\"Total sum:\", tp + tn + fp + fn)\n",
    "print(\"Accuracy:\", (tp + tn) / (tp + tn + fp + fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2ae36fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    227451\n",
      "1       394\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b475bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
